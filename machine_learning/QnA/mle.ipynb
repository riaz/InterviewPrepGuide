{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does Maximum Likelihood Extimation Mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution by maximizing the likelihood function. Here is a basic explanation:\n",
    "\n",
    "The likelihood function measures the probability of observing the actual data as a function of the unknown parameters. For a given statistic model and observed data, we can construct a likelihood function based on the probability distribution specified by the model. \n",
    "\n",
    "The maximum likelihood estimation selects the values of the model parameters that maximize the likelihood function. Intuitively, these parameter values imply the highest probability of generating the observed data.\n",
    "\n",
    "Mathematically, if we observe data X1, X2, ..., Xn which follows a distribution f(X|θ) where θ denotes the parameters, then the likelihood function is: \n",
    "\n",
    "L(θ|X1, ..., Xn) = f(X1|θ) * f(X2|θ) * ... * f(Xn|θ)\n",
    "\n",
    "To obtain the MLEs, we take the derivative of the log likelihood function with respect to each parameter, set them to zero and solve the likelihood equations. The resulting parameter estimates are the maximum likelihood estimates.  \n",
    "\n",
    "Some properties of MLEs:\n",
    "- Consistent - converge to the true parameter values as sample size increases\n",
    "- Asymptotically normal - approximates a normal distribution for large sample sizes\n",
    "- Invariant - doesn't depend on reparameterization \n",
    "\n",
    "In summary, MLE finds the parameter values that give the observed data the greatest probability of occurrence based on the chosen model. By maximizing the fit of the model to the data, it provides optimal parameter \n",
    "estimates.\n",
    "\n",
    "If the set of possible values of theta is not finite, we need to optimize using a numerical optimzation routine such as gradient descent.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between the hyperplane or decision boundries in SVM vs Linerar Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The concepts of hyperplanes and decision boundaries are fundamental in understanding both Support Vector Machines (SVM) and Linear Regression, but they serve different purposes and are used differently in each model.\n",
    "\n",
    "1. Support Vector Machine (SVM)\n",
    "Hyperplane in SVM: In the context of SVM, a hyperplane is used as a decision boundary that separates different classes in the data. SVM is a classification algorithm, and its primary goal is to find the hyperplane that best divides a dataset into classes.\n",
    "\n",
    "Properties:\n",
    "Maximizing Margin: The hyperplane in SVM is chosen to maximize the margin, which is the distance between the hyperplane and the nearest data points of each class (these points are called support vectors).\n",
    "Dimensionality: The hyperplane has one dimension less than the feature space. For instance, in a 2D space, it's a line; in a 3D space, it's a plane, and so on.\n",
    "Non-linear Boundaries: For non-linearly separable data, SVM uses kernel functions to transform the data into a higher-dimensional space where a hyperplane can be used for separation.\n",
    "\n",
    "2. Linear Regression\n",
    "Decision Boundary in Linear Regression: Linear regression is a regression algorithm, not a classification one. Typically, it doesn't have a concept of a \"decision boundary\" in the same way as classification models. However, if used for classification (like in Logistic Regression), the decision boundary determines the threshold at which the continuous output of the regression is mapped to a particular class.\n",
    "Properties:\n",
    "Predictive Line/Plane: In linear regression, you fit a line (in 2D), plane (in 3D), or hyperplane (in higher dimensions) that best predicts the continuous outcome variable based on the input features.\n",
    "Continuous Outcome: Unlike SVM, which deals with discrete class labels, linear regression deals with continuous outcomes.\n",
    "\n",
    "Error Minimization: The objective in linear regression is to minimize the difference (error) between the predicted values and the actual values.\n",
    "Key Differences:\n",
    "Purpose:\n",
    "\n",
    "SVM: Classification task, separating classes.\n",
    "Linear Regression: Regression task, predicting continuous outcomes.\n",
    "Nature of Output:\n",
    "\n",
    "SVM: Discrete classes.\n",
    "Linear Regression: Continuous values.\n",
    "Objective Function:\n",
    "\n",
    "SVM: Maximizing the margin between classes.\n",
    "Linear Regression: Minimizing the error between predicted and actual values.\n",
    "Decision Boundary Application:\n",
    "\n",
    "SVM: A boundary for class separation.\n",
    "Linear Regression: Not typically used for boundary decisions, but if so, for thresholding in classification.\n",
    "Understanding these differences is crucial for choosing the right algorithm for a given data problem and interpreting the results appropriately.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Over-fitting. What steps will you take to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Overfitting is a common problem in machine learning and statistical modeling where a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means the model is too complex and fits the training data too closely, capturing noise and fluctuations in the data as if they were meaningful patterns. As a result, it performs poorly on unseen data, as it fails to generalize from the training data to a broader range of inputs.\n",
    "\n",
    "Causes of Overfitting\n",
    "Too Many Features: Having too many features relative to the number of observations can lead to overfitting.\n",
    "Model Complexity: Excessively complex models with too many parameters can \"memorize\" the training data.\n",
    "Lack of Data: Overfitting is more likely when you have a small amount of training data.\n",
    "Noisy Data: Data with a lot of noise can lead to models capturing the noise as if it were a signal.\n",
    "Steps to Prevent Overfitting\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to ensure that the model's performance is consistent across different subsets of the training data.\n",
    "\n",
    "Train with More Data: More data can help algorithms detect the signal better and generalize well.\n",
    "\n",
    "Feature Selection: Reduce the number of irrelevant or partially relevant features. Techniques like backward elimination, forward selection, or using models with built-in feature selection (like LASSO regression) can help.\n",
    "\n",
    "Regularization: Implement regularization methods (like L1 or L2 regularization) which penalize the complexity of the model. This encourages simpler models that may generalize better.\n",
    "\n",
    "Simplifying the Model: Use a simpler model with fewer parameters. Sometimes, simpler models are more robust and generalize better.\n",
    "\n",
    "Pruning (in Decision Trees): Reduce the size of decision trees by cutting off branches that use features which add little predictive power.\n",
    "\n",
    "Early Stopping: In gradient descent-based algorithms, stop training before the learner passes the point where it begins to overfit.\n",
    "\n",
    "Ensembling Techniques: Use methods like bagging and boosting which combine the predictions from multiple models to improve robustness.\n",
    "\n",
    "Validation Metrics: Focus on the right metrics that are more robust against overfitting. For instance, AUC-ROC for classification problems.\n",
    "\n",
    "Data Augmentation (for Neural Networks): In deep learning, particularly in computer vision tasks, artificially increasing the size of your training dataset using transformations can help.\n",
    "\n",
    "Dropout (for Neural Networks): In neural networks, dropout is a regularization technique where randomly selected neurons are ignored during training, which helps in preventing overfitting.\n",
    "\n",
    "Avoiding Data Leakage: Ensure that information from outside the training dataset does not influence the model.\n",
    "\n",
    "By implementing these strategies, you can balance the model's ability to learn from the training data while also maintaining its ability to generalize to new, unseen data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Under-fitting. What steps will you take to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Underfitting occurs in machine learning and statistics when a model is too simple to capture the underlying pattern in the data. This simplicity leads to poor performance on both the training data and unseen data. Underfitting is often a result of an overly simplistic model, not enough training, or neglecting to capture important trends and patterns in the data.\n",
    "\n",
    "Causes of Underfitting\n",
    "Overly Simplistic Model: A model that is too simple may not have enough capacity or flexibility to learn the data's structure.\n",
    "Insufficient Training: Not training the model long enough can lead to underfitting, as the model doesn't get the chance to learn the data adequately.\n",
    "Poor Feature Selection: If important features are missing or irrelevant features are used, the model may not accurately predict outcomes.\n",
    "Noisy or Incomplete Data: Data quality issues can hinder a model's ability to learn effectively.\n",
    "Steps to Prevent Underfitting\n",
    "Increasing Model Complexity: Use a more complex model with more parameters. This can be achieved by increasing the number of layers or neurons in a neural network, adding more features, or choosing a more sophisticated model.\n",
    "\n",
    "Feature Engineering: Improve the model's performance by creating new features or transforming existing ones to better capture the data's underlying patterns.\n",
    "\n",
    "More Training: Ensure that the model is trained sufficiently. Sometimes, simply increasing the number of training iterations or epochs can help the model learn better.\n",
    "\n",
    "Reducing Regularization: If regularization (like L1 or L2) is applied, reducing its strength can allow the model more flexibility to fit the data.\n",
    "\n",
    "Data Quality Improvement: Clean the data by handling missing values, removing noise, and ensuring that the data is complete and representative of the problem domain.\n",
    "\n",
    "Parameter Tuning: Optimize the model's hyperparameters through techniques like grid search or random search to find the best combination for your model and data.\n",
    "\n",
    "Enriching Data: If possible, add more relevant data or use data augmentation techniques to provide the model with more information to learn from.\n",
    "\n",
    "Algorithm Tuning: Sometimes, the choice of algorithm might not be suitable for the given data or problem. Experimenting with different algorithms can help find a better fit.\n",
    "\n",
    "Reducing Dropout (in Neural Networks): If dropout is used in neural networks, reducing the dropout rate can allow the network to use more of its capacity.\n",
    "\n",
    "Cross-Validation: Use cross-validation to ensure that the model performs well across different subsets of the dataset and is not just underfitting on the specific training set.\n",
    "\n",
    "Revisiting Preprocessing Steps: Ensure that data preprocessing steps like normalization or scaling are appropriate and not overly distorting the data.\n",
    "\n",
    "By addressing these factors, you can help your model achieve a better balance between bias and variance, fitting the training data adequately while maintaining the ability to generalize to new data.```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the loss function and cost funtion for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Loss function - square erros\n",
    "\n",
    "    Cost function - mean square errors\n",
    "\n",
    "    Objective - Optimizing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the loss function and cost function for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function - Log-Likelihood\n",
    "Objective - Maximize the Log Likelihood\n",
    "\n",
    "There is no closed form solution to the abobve optimization oproblem and hence gradient descent needs to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does closed form solution mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"closed-form solution\" in mathematics and related disciplines like statistics, economics, and engineering refers to any solution to a problem, usually an equation or system of equations, that can be expressed analytically in a finite number of standard operations and functions. Essentially, it's a way to solve a problem with an explicit formula or expression.\n",
    "\n",
    "Characteristics of Closed-Form Solutions:\n",
    "Explicit Expression: The solution can be expressed in terms of elementary functions such as polynomials, exponentials, logarithms, trigonometric functions, etc.\n",
    "\n",
    "Finite Operations: The solution is obtained using a finite number of operations. It's not an infinite series, an integral that cannot be simplified into elementary functions, or a limit that cannot be explicitly computed.\n",
    "\n",
    "Direct Computation: The solution allows for direct computation of the answer without the need for iterative methods or numerical approximation.\n",
    "\n",
    "Precise and Exact: Closed-form solutions give a precise and exact answer, as opposed to numerical methods which might give an approximate solution.\n",
    "\n",
    "Examples:\n",
    "Algebra: The quadratic formula is a closed-form solution for the roots of a quadratic equation.\n",
    "Finance: The formula for the future value of a compound interest investment is a closed-form solution.\n",
    "Statistics: The formula for the mean of a sample is a closed-form solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Attention is All you need Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The \"Attention Is All You Need\" paper, published in 2017 by researchers at Google, introduced the Transformer model, which has since revolutionized the field of natural language processing (NLP). This model's key innovation was its reliance solely on attention mechanisms, diverging from the then-standard use of recurrent neural networks (RNNs) or convolutional neural networks (CNNs) for processing sequential data.\n",
    "\n",
    "Key Components of the Transformer\n",
    "Attention Mechanism\n",
    "\n",
    "The core of the Transformer is the \"self-attention\" mechanism. It allows the model to weigh the importance of different parts of the input data differently. This is more efficient than the sequential processing in RNNs and allows for parallel processing.\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "The Transformer follows an encoder-decoder structure. The encoder maps an input sequence to a sequence of continuous representations, which the decoder then uses to generate an output sequence.\n",
    "Positional Encoding\n",
    "\n",
    "Since the Transformer doesn't process data sequentially, it uses positional encodings to give the model information about the order of the words in the sentence.\n",
    "Loss Function\n",
    "The loss function used in the Transformer model for training is typically cross-entropy loss, a standard choice for classification tasks, including sequence-to-sequence models like the Transformer. In the context of NLP, this involves predicting the next word in a sequence given the previous words.\n",
    "Optimizer\n",
    "The original paper used the Adam optimizer with specific learning rate scheduling. The learning rate increases linearly for the first warm-up steps and then decreases proportionally to the inverse square root of the step number.\n",
    "Additional Critical Information\n",
    "Layer Normalization\n",
    "\n",
    "Each sub-block in the encoder and decoder contains a layer normalization step, which stabilizes the training process.\n",
    "Multi-Head Attention\n",
    "\n",
    "The model uses multi-head attention to allow the model to jointly attend to information from different representation subspaces at different positions.\n",
    "Feed-Forward Networks\n",
    "\n",
    "In both the encoder and decoder, each layer contains a fully connected feed-forward network applied to each position separately and identically.\n",
    "Regularization\n",
    "\n",
    "Dropout is used for regularization in various parts of the model.\n",
    "Scaling Factor in Attention\n",
    "\n",
    "A scaling factor (the square root of the dimension of the key vectors) is used in the dot products of the attention to prevent extremely small gradients.\n",
    "No Recurrence or Convolution\n",
    "\n",
    "The Transformer model is unique in its lack of recurrence and convolution, relying entirely on attention mechanisms.\n",
    "BPE Tokenization\n",
    "\n",
    "The paper used Byte Pair Encoding (BPE), a form of tokenization that is more efficient than word-level tokenization.\n",
    "Application\n",
    "\n",
    "While the Transformer was originally designed for translation tasks, its architecture has been foundational for subsequent models in various NLP tasks.\n",
    "The \"Attention Is All You Need\" paper was groundbreaking in its simplicity and efficiency, leading to the development of models like BERT, GPT, and others that dominate the NLP field today. The Transformer's ability to process sequences in parallel and its effective handling of long-range dependencies marked a significant shift from previous sequence modeling approaches.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what is Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Multi-head attention is a core component of the Transformer model, as introduced in the \"Attention is All You Need\" paper. This mechanism enhances the ability of the model to focus on different parts of the input sequence and extract various types of information simultaneously. To understand multi-head attention, it's essential first to grasp the concept of scaled dot-product attention, upon which it builds.\n",
    "\n",
    "Scaled Dot-Product Attention\n",
    "The basic form of attention used in the Transformer is the scaled dot-product attention. It involves three components: queries (Q), keys (K), and values (V), typically derived from the input embeddings. The attention mechanism works as follows:\n",
    "\n",
    "Dot Products of Queries and Keys: Compute the dot products of the queries with all keys to get the weights representing how much focus to put on other parts of the input sequence.\n",
    "\n",
    "\n",
    "Scaling: Scale the dot products by \n",
    "\n",
    "1/sqrt(dk)\n",
    "​\n",
    " \n",
    "​\n",
    "  is the dimension of the key vectors. This scaling helps in stabilizing the gradients during training.\n",
    "\n",
    "\n",
    "Softmax Function: Apply the softmax function to the scaled dot products to obtain the weights on the values. This step turns the weights into a probability distribution.\n",
    "\n",
    "\n",
    "Output: Multiply the weights with the values and sum them up to produce the final output of the attention layer.\n",
    "\n",
    "\n",
    ">  sigma(softmax(scale(dot(K, Q), 1/sqrt(Dk))) * V)\n",
    "\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "Multi-head attention extends the scaled dot-product attention by running through the attention mechanism multiple times in parallel. The key idea is to have each 'head' of the multi-head attention focus on different parts of the input sequence. Here's how it works:\n",
    "\n",
    "Linear Projections: Each head applies different linear transformations (using trainable weights) to the queries, keys, and values before feeding them into the attention mechanism. This results in each head having a different set of Q, K, and V.\n",
    "\n",
    "Parallel Attention Layers: The scaled dot-product attention is applied in parallel to these transformed Q, K, and V for each head. As a result, each head captures different aspects or relationships within the data.\n",
    "\n",
    "Concatenation: The outputs of all heads are concatenated.\n",
    "\n",
    "Final Linear Transformation: The concatenated output then goes through another linear transformation to produce the final output of the multi-head attention layer.\n",
    "\n",
    "Advantages of Multi-Head Attention\n",
    "Richer Representations: Each head can potentially learn to focus on different features or parts of the sequence, leading to richer representations of the input.\n",
    "Flexibility: The model can simultaneously attend to information from different representation subspaces, allowing for more complex patterns to be learned.\n",
    "Parallelization: Heads operate in parallel, providing computational efficiency.\n",
    "Applications\n",
    "Multi-head attention has proven particularly effective in tasks involving complex relationships and dependencies in data, such as language modeling, translation, and even tasks beyond NLP like image processing in recent advancements.\n",
    "\n",
    "In summary, multi-head attention allows the Transformer model to simultaneously process information in multiple representation spaces, enabling it to capture a wide array of relationships in the data, which is crucial for understanding the complexities of language and other sequential data.\n",
    "```````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the various ways to tokenize for Language Models. Describes their similarity and differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Here is a comparison of different tokenization methods commonly used for language modeling:\n",
    "\n",
    "Word Tokenization\n",
    "- Splits text into words using spaces and punctuation as delimiters. \n",
    "- Words maintain semantic meaning, but vocabulary size can be very large.\n",
    "\n",
    "Character Tokenization\n",
    "- Splits text into individual characters.\n",
    "- Vocabulary size is small, but loses word-level semantics.\n",
    "\n",
    "Subword Tokenization\n",
    "- Splits words into subword units, like morphemes. BPE (Byte Pair Encoding) is a common approach.  \n",
    "- Maintains some semantic meaning unlike characters. Vocabulary size is smaller than words.\n",
    "\n",
    "WordPiece Tokenization\n",
    "- Data-driven approach to learn the optimal subword vocabulary.\n",
    "- Balances vocabulary size vs capturing semantics. Widely used in models like BERT.\n",
    "\n",
    "SentencePiece Tokenization  \n",
    "- Unsupervised text tokenizer and detokenizer by Google.\n",
    "- Builds subword vocabulary based on language model objectives.\n",
    "\n",
    "The main difference lies in how much they preserve semantic meaning vs reducing vocabulary size. Words maintain complete semantics but don't address rare/unknown words. Characters have small vocabulary but lose semantics. Subword tokenization aims to get best of both.\n",
    "\n",
    "The choice depends on computational constraints and data characteristics. For morphologically rich languages, subword is more effective. SentencePiece and WordPiece learn the optimal mapping data-driven rather than heuristics like BPE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the loss function of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The BERT model uses two types of loss functions during pre-training:\n",
    "\n",
    "Masked Language Modeling (MLM) Loss:\n",
    "- Randomly masks 15% of input tokens.\n",
    "- Passes corrupted input to model to predict masked tokens. \n",
    "- Cross entropy loss between predicted token and original token calculated.\n",
    "\n",
    "Next Sentence Prediction (NSP) Loss:  \n",
    "- Classification loss predicting if sentence B follows sentence A.\n",
    "- Two sentences fed as input, 50% chance of B actually following A.\n",
    "- NSP loss calculated from classifier output probability.\n",
    "\n",
    "The final BERT pre-training loss is a sum of the mean MLM loss + mean NSP loss over each sequence+mask in the input batch. This joint loss forces the model to learn both robust language modeling of text and sentence relationships.\n",
    "\n",
    "So in summary:\n",
    "\n",
    "1. MLM loss: cross entropy on predicting masked input tokens  \n",
    "2. NSP loss: classification loss of next sentence prediction\n",
    "3. Final Loss = MLM loss + NSP loss\n",
    "\n",
    "The trained BERT model optimizes this final joint objective function to learn powerful contextual word representations that can be fine-tuned for downstream tasks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain about BERT Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The BERT (Bidirectional Encoder Representations from Transformers) model, introduced by Google, is designed for a wide range of natural language processing tasks. The loss function used in BERT depends on the specific task for which it is being fine-tuned. However, during its pre-training phase, BERT uses two primary loss functions:\n",
    "\n",
    "1. **Masked Language Model (MLM) Loss**: \n",
    "   - In the MLM task, a certain percentage of the input tokens are randomly masked, and the model's objective is to predict the original identity of these masked tokens. \n",
    "   - The loss function used for the MLM task is the Cross-Entropy Loss. Specifically, it calculates the cross-entropy loss between the predicted probabilities of the masked tokens and the actual identities of these tokens. \n",
    "   - This process allows BERT to understand the context on both sides of a masked token, leading to a deep bidirectional understanding of the language context.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP) Loss**: \n",
    "   - In the NSP task, the model is given pairs of sentences as input and is trained to predict whether the second sentence in the pair is the subsequent sentence in the original document (labelled as \"IsNext\") or a random sentence from the corpus (labelled as \"NotNext\").\n",
    "   - The loss function here is also Cross-Entropy Loss, but it is applied to the binary classification task of predicting whether the sentences are consecutive or not.\n",
    "\n",
    "### Overall Pre-training Loss\n",
    "\n",
    "- The total loss for the pre-training of BERT is the sum of the MLM loss and the NSP loss.\n",
    "\n",
    "### Fine-tuning Loss\n",
    "\n",
    "- When BERT is fine-tuned for specific downstream tasks like sentiment analysis, question answering, or named entity recognition, the loss function is chosen according to the nature of the task. For instance, cross-entropy loss is commonly used for classification tasks, and span-based models like those used in question answering might use a combination of losses specific to the start and end positions of the answer span.\n",
    "\n",
    "### Importance of Loss Functions in BERT\n",
    "\n",
    "- The choice of these loss functions during pre-training is crucial as it enables BERT to learn a rich and nuanced language representation, making it effective for a wide range of language understanding tasks. The MLM loss, in particular, is innovative in that it allows for deep bidirectional training in contrast to previous models, which were typically unidirectional.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
